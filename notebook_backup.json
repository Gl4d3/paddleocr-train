{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Meter Reading Detection Model Training\n",
    "# \n",
    "# This notebook provides an interactive way to train detection models for meter reading using PaddleOCR.\n",
    "\n",
    "# ## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53545370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import subprocess\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image, clear_output\n",
    "\n",
    "# Ensure we're in the PaddleOCR root directory\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"../..\"))\n",
    "os.chdir(ROOT_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if key directories exist\n",
    "assert os.path.exists('ppocr'), \"Not in PaddleOCR root directory!\"\n",
    "assert os.path.exists('tools'), \"PaddleOCR tools directory not found!\"\n",
    "\n",
    "# ## 2. Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Default dataset paths (can be changed)\n",
    "DET_DATASET_DIR = \"dataset/det_dataset_1\"\n",
    "TRAIN_DATA_DIR = \"train_data/meter_detection\"\n",
    "\n",
    "# Cell for selecting dataset (edit these variables)\n",
    "det_dataset_dir = DET_DATASET_DIR  # Change this to your dataset directory\n",
    "train_data_dir = TRAIN_DATA_DIR    # Change this to your train data directory\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(det_dataset_dir):\n",
    "    print(f\"Warning: Dataset directory {det_dataset_dir} not found!\")\n",
    "else:\n",
    "    # Count images\n",
    "    image_count = len(glob.glob(os.path.join(det_dataset_dir, \"images/*.jpg\")))\n",
    "    print(f\"Found {image_count} images in {det_dataset_dir}/images/\")\n",
    "    \n",
    "    # Check annotation files\n",
    "    train_anno = os.path.join(det_dataset_dir, \"train_verified.txt\")\n",
    "    test_anno = os.path.join(det_dataset_dir, \"test_verified.txt\")\n",
    "    \n",
    "    if os.path.exists(train_anno):\n",
    "        with open(train_anno, 'r') as f:\n",
    "            train_lines = len(f.readlines())\n",
    "        print(f\"Training annotations: {train_lines} lines\")\n",
    "    else:\n",
    "        print(f\"Warning: Training annotations file {train_anno} not found!\")\n",
    "        \n",
    "    if os.path.exists(test_anno):\n",
    "        with open(test_anno, 'r') as f:\n",
    "            test_lines = len(f.readlines())\n",
    "        print(f\"Test annotations: {test_lines} lines\")\n",
    "    else:\n",
    "        print(f\"Warning: Test annotations file {test_anno} not found!\")\n",
    "\n",
    "# ## 3. Data Preparation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64856471",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data preparation and visualization\n",
    "# Check if training data exists or needs to be created\n",
    "train_label = os.path.join(train_data_dir, \"train_label.txt\")\n",
    "test_label = os.path.join(train_data_dir, \"test_label.txt\")\n",
    "\n",
    "if not os.path.exists(train_label) or not os.path.exists(test_label):\n",
    "    print(\"Training data not found. Need to prepare from detection dataset.\")\n",
    "    prep_cmd = f\"\"\"\n",
    "    python tools/train_val_split.py \\\n",
    "        --dataset_dir={det_dataset_dir} \\\n",
    "        --output_dir={train_data_dir} \\\n",
    "        --train_ratio=0.8\n",
    "    \"\"\"\n",
    "    print(f\"Running: {prep_cmd}\")\n",
    "    \n",
    "    # Uncomment to actually run the preparation\n",
    "    # subprocess.run(prep_cmd, shell=True)\n",
    "    print(\"Data preparation command is ready. Uncomment to execute.\")\n",
    "else:\n",
    "    print(\"Training data already prepared.\")\n",
    "    # Show some statistics\n",
    "    with open(train_label, 'r') as f:\n",
    "        train_lines = len(f.readlines())\n",
    "    with open(test_label, 'r') as f:\n",
    "        test_lines = len(f.readlines())\n",
    "    print(f\"Training samples: {train_lines}\")\n",
    "    print(f\"Test samples: {test_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e37627e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Visualize some training samples\n",
    "def visualize_samples(samples_file, image_dir, max_samples=5):\n",
    "    \"\"\"Visualize samples from the dataset\"\"\"\n",
    "    if not os.path.exists(samples_file):\n",
    "        print(f\"File not found: {samples_file}\")\n",
    "        return\n",
    "    \n",
    "    with open(samples_file, 'r') as f:\n",
    "        samples = f.readlines()[:max_samples]\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        parts = sample.strip().split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            img_path = parts[0]\n",
    "            full_path = os.path.join(image_dir, img_path)\n",
    "            \n",
    "            if os.path.exists(full_path):\n",
    "                print(f\"Sample {i+1}: {img_path}\")\n",
    "                display(Image(filename=full_path, width=500))\n",
    "            else:\n",
    "                print(f\"Image not found: {full_path}\")\n",
    "\n",
    "# Uncomment to visualize samples\n",
    "# visualize_samples(train_label, \"train_data/meter_detection/images\")\n",
    "\n",
    "# ## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f250a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training configuration\n",
    "# You can modify these parameters\n",
    "config = {\n",
    "    # Basic parameters\n",
    "    \"mode\": \"teacher\",  # \"teacher\" or \"student\"\n",
    "    \"gpu_ids\": \"0\",     # Comma-separated GPU IDs\n",
    "    \"max_epochs\": 200,  # Maximum epochs\n",
    "    \n",
    "    # Dataset parameters\n",
    "    \"det_dataset_dir\": det_dataset_dir,\n",
    "    \"train_data_dir\": train_data_dir,\n",
    "    \n",
    "    # Learning parameters\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 8,\n",
    "    \"save_epoch_step\": 5,\n",
    "    \"eval_batch_step\": 500,\n",
    "    \n",
    "    # Model parameters\n",
    "    \"pretrained_model\": \"pretrain_models/det_mv3_db_v2.0_train/best_accuracy\",\n",
    "    \"model_save_dir\": \"./model_training/det_train/output/meter_teacher\",  # For teacher mode\n",
    "}\n",
    "\n",
    "# Update model_save_dir based on mode\n",
    "if config[\"mode\"] == \"student\":\n",
    "    config[\"pretrained_model\"] = \"./model_training/det_train/output/meter_teacher/best_accuracy\"\n",
    "    config[\"model_save_dir\"] = \"./model_training/det_train/output/meter_student\"\n",
    "\n",
    "# Print configuration\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ## 5. Download Pretrained Model (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce499b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check and download pretrained model if needed\n",
    "def check_download_pretrained(pretrained_path):\n",
    "    \"\"\"Check if pretrained model exists, download if needed\"\"\"\n",
    "    if os.path.exists(pretrained_path) or os.path.exists(f\"{pretrained_path}.pdparams\"):\n",
    "        print(f\"Pretrained model found at {pretrained_path}\")\n",
    "        return True\n",
    "    \n",
    "    # For built-in pretrained models, download them\n",
    "    if pretrained_path.startswith(\"pretrain_models/\"):\n",
    "        model_name = pretrained_path.split(\"/\")[1]\n",
    "        print(f\"Downloading pretrained model: {model_name}\")\n",
    "        \n",
    "        # Different download URLs based on model type\n",
    "        if \"det_mv3_db\" in model_name:\n",
    "            url = \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/en/det_mv3_db_v2.0_train.tar\"\n",
    "        else:\n",
    "            print(f\"Unknown pretrained model: {model_name}\")\n",
    "            return False\n",
    "        \n",
    "        # Create directory and download\n",
    "        os.makedirs(\"pretrain_models\", exist_ok=True)\n",
    "        download_cmd = f\"wget {url} -P pretrain_models/\"\n",
    "        extract_cmd = f\"tar -xf pretrain_models/{os.path.basename(url)} -C pretrain_models/\"\n",
    "        \n",
    "        print(f\"Running: {download_cmd}\")\n",
    "        subprocess.run(download_cmd, shell=True)\n",
    "        \n",
    "        print(f\"Running: {extract_cmd}\")\n",
    "        subprocess.run(extract_cmd, shell=True)\n",
    "        \n",
    "        return os.path.exists(pretrained_path) or os.path.exists(f\"{pretrained_path}.pdparams\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Check pretrained model\n",
    "if not check_download_pretrained(config[\"pretrained_model\"]):\n",
    "    print(f\"Warning: Pretrained model {config['pretrained_model']} not found and couldn't be downloaded.\")\n",
    "    print(\"You may need to download it manually or specify a different pretrained model.\")\n",
    "\n",
    "# ## 6. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate training command\n",
    "def generate_training_cmd(config):\n",
    "    \"\"\"Generate the training command based on configuration\"\"\"\n",
    "    cmd = f\"\"\"python -m paddle.distributed.launch --gpus='{config[\"gpu_ids\"]}' tools/train.py \\\n",
    "        -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_teacher.yml \\\n",
    "        -o Global.pretrained_model={config[\"pretrained_model\"]} \\\n",
    "        Global.save_model_dir={config[\"model_save_dir\"]} \\\n",
    "        Train.dataset.data_dir={config[\"train_data_dir\"]} \\\n",
    "        Train.dataset.label_file_list=['{os.path.join(config[\"train_data_dir\"], \"train_label.txt\")}'] \\\n",
    "        Eval.dataset.data_dir={config[\"train_data_dir\"]} \\\n",
    "        Eval.dataset.label_file_list=['{os.path.join(config[\"train_data_dir\"], \"test_label.txt\")}'] \\\n",
    "        Train.loader.batch_size_per_card={config[\"batch_size\"]} \\\n",
    "        Optimizer.lr.values=[{config[\"learning_rate\"]},{config[\"learning_rate\"]/10}] \\\n",
    "        Global.epoch_num={config[\"max_epochs\"]} \\\n",
    "        Global.save_epoch_step={config[\"save_epoch_step\"]} \\\n",
    "        Global.eval_batch_step=[0, {config[\"eval_batch_step\"]}]\"\"\"\n",
    "    return cmd\n",
    "\n",
    "training_cmd = generate_training_cmd(config)\n",
    "print(\"Generated training command:\")\n",
    "print(training_cmd)\n",
    "\n",
    "# Uncomment the following to execute training\n",
    "# def run_with_output(cmd):\n",
    "#     \"\"\"Run a command and display output in real-time\"\"\"\n",
    "#     process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "#     for line in iter(process.stdout.readline, ''):\n",
    "#         print(line, end='')\n",
    "#     return process.wait()\n",
    "# \n",
    "# print(\"Starting training...\")\n",
    "# run_with_output(training_cmd)\n",
    "\n",
    "# ## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation after training\n",
    "def generate_eval_cmd(config):\n",
    "    \"\"\"Generate evaluation command\"\"\"\n",
    "    model_dir = config[\"model_save_dir\"]\n",
    "    eval_cmd = f\"\"\"python tools/eval.py \\\n",
    "        -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_teacher.yml \\\n",
    "        -o Global.checkpoints={model_dir}/best_accuracy \\\n",
    "        Eval.dataset.data_dir={config[\"train_data_dir\"]} \\\n",
    "        Eval.dataset.label_file_list=['{os.path.join(config[\"train_data_dir\"], \"test_label.txt\")}']\"\"\"\n",
    "    return eval_cmd\n",
    "\n",
    "eval_cmd = generate_eval_cmd(config)\n",
    "print(\"Evaluation command:\")\n",
    "print(eval_cmd)\n",
    "\n",
    "# Uncomment to run evaluation\n",
    "# print(\"Running evaluation...\")\n",
    "# run_with_output(eval_cmd)\n",
    "\n",
    "# ## 8. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run inference on a sample image\n",
    "def generate_infer_cmd(config, sample_image):\n",
    "    \"\"\"Generate inference command for a sample image\"\"\"\n",
    "    model_dir = config[\"model_save_dir\"]\n",
    "    infer_cmd = f\"\"\"python tools/infer_det.py \\\n",
    "        -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_teacher.yml \\\n",
    "        -o Global.infer_img={sample_image} \\\n",
    "        Global.checkpoints={model_dir}/best_accuracy\"\"\"\n",
    "    return infer_cmd\n",
    "\n",
    "# Choose a sample image (modify path as needed)\n",
    "sample_image = os.path.join(det_dataset_dir, \"images/sample.jpg\")\n",
    "if not os.path.exists(sample_image):\n",
    "    # Try to find any image\n",
    "    sample_images = glob.glob(os.path.join(det_dataset_dir, \"images/*.jpg\"))\n",
    "    if sample_images:\n",
    "        sample_image = sample_images[0]\n",
    "        print(f\"Using sample image: {sample_image}\")\n",
    "    else:\n",
    "        print(\"No sample images found for inference.\")\n",
    "        sample_image = None\n",
    "\n",
    "if sample_image:\n",
    "    infer_cmd = generate_infer_cmd(config, sample_image)\n",
    "    print(\"Inference command:\")\n",
    "    print(infer_cmd)\n",
    "    \n",
    "    # Uncomment to run inference\n",
    "    # print(\"Running inference...\")\n",
    "    # run_with_output(infer_cmd)\n",
    "    # \n",
    "    # # Display the inference result if available\n",
    "    # result_image = \"inference_results/det_res.jpg\"\n",
    "    # if os.path.exists(result_image):\n",
    "    #     display(Image(filename=result_image))\n",
    "    # else:\n",
    "    #     print(f\"Result image not found: {result_image}\")\n",
    "\n",
    "# ## 9. Export Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bddfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export the model for deployment\n",
    "def generate_export_cmd(config):\n",
    "    \"\"\"Generate model export command\"\"\"\n",
    "    model_dir = config[\"model_save_dir\"]\n",
    "    export_dir = os.path.join(model_dir, \"inference\")\n",
    "    export_cmd = f\"\"\"python tools/export_model.py \\\n",
    "        -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_teacher.yml \\\n",
    "        -o Global.pretrained_model={model_dir}/best_accuracy \\\n",
    "        Global.save_inference_dir={export_dir}\"\"\"\n",
    "    return export_cmd\n",
    "\n",
    "export_cmd = generate_export_cmd(config)\n",
    "print(\"Export command:\")\n",
    "print(export_cmd)\n",
    "\n",
    "# Uncomment to export model\n",
    "# print(\"Exporting model...\")\n",
    "# run_with_output(export_cmd) "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
